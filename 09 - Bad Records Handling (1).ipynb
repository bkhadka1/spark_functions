{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad278353-7166-4b04-9841-f0dc5b35e244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>EmpID</th><th>EmpCount</th><th>ProductionUnit</th><th>Expenses</th></tr></thead><tbody><tr><td>January</td><td>1</td><td>78</td><td>3245.76</td><td>1125.43</td></tr><tr><td>February</td><td>2</td><td>93</td><td>4678.24</td><td>1458.21</td></tr><tr><td>March</td><td>3</td><td>57</td><td>1987.91</td><td>892.34</td></tr><tr><td>April</td><td>4</td><td>65</td><td>3562.51</td><td>1234.78</td></tr><tr><td>May</td><td>5</td><td>82</td><td>2894.37</td><td>987.65</td></tr><tr><td>June</td><td>6</td><td>74</td><td>1245.89</td><td>765.43</td></tr><tr><td>July</td><td>7</td><td>68</td><td>3987.32</td><td>hi</td></tr><tr><td>August</td><td>8</td><td>89</td><td>2312.45</td><td>1100.56</td></tr><tr><td>September</td><td>9</td><td>73</td><td>1789.34</td><td>800.21</td></tr><tr><td>October</td><td>10</td><td>96</td><td>4321.76</td><td>1650.32</td></tr><tr><td>November</td><td>11</td><td>53</td><td>1345.78</td><td>700.43</td></tr><tr><td>December</td><td>12</td><td>61</td><td>2789.12</td><td>1200.87</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "January",
         1,
         78,
         3245.76,
         "1125.43"
        ],
        [
         "February",
         2,
         93,
         4678.24,
         "1458.21"
        ],
        [
         "March",
         3,
         57,
         1987.91,
         "892.34"
        ],
        [
         "April",
         4,
         65,
         3562.51,
         "1234.78"
        ],
        [
         "May",
         5,
         82,
         2894.37,
         "987.65"
        ],
        [
         "June",
         6,
         74,
         1245.89,
         "765.43"
        ],
        [
         "July",
         7,
         68,
         3987.32,
         "hi"
        ],
        [
         "August",
         8,
         89,
         2312.45,
         "1100.56"
        ],
        [
         "September",
         9,
         73,
         1789.34,
         "800.21"
        ],
        [
         "October",
         10,
         96,
         4321.76,
         "1650.32"
        ],
        [
         "November",
         11,
         53,
         1345.78,
         "700.43"
        ],
        [
         "December",
         12,
         61,
         2789.12,
         "1200.87"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmpID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "EmpCount",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ProductionUnit",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Expenses",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df= spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(\"/FileStore/files/emp_test-1.csv\")\n",
    "display(df) # Read a CSV file into a Spark DataFrame 'df'.\n",
    "# - .format(\"csv\"): specifies the file format as CSV\n",
    "# - .option(\"header\", \"true\"): treat the first row as column headers\n",
    "# - .option(\"inferschema\", \"true\"): automatically infer the data types of columns\n",
    "# - .load(\"/FileStore/files/emp_test-1.csv\"): path to the CSV file in Databricks FileStore\n",
    "# Display the DataFrame in an interactive tabular view in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d7d49a-b404-47e9-a67b-cfbdde7b419d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Schema"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType # Import necessary PySpark SQL types for defining a schema.\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Month\", StringType(), True),\n",
    "    StructField(\"EmpID\", IntegerType(), True),\n",
    "    StructField(\"EmpCount\", IntegerType(), True),\n",
    "    StructField(\"ProductionUnit\", FloatType(), True),\n",
    "    StructField(\"Expenses\", FloatType(), True),  # Change from IntegerType() to FloatType()\n",
    "    StructField(\"_corrupt_record\", StringType(), True),  # Change from FloatType() to StringType()\n",
    "]) # Define a custom schema for a DataFrame using StructType and StructField.\n",
    "# - 'Month': StringType, allows nulls\n",
    "# - 'EmpID': IntegerType, allows nulls\n",
    "# - 'EmpCount': IntegerType, allows nulls\n",
    "# - 'ProductionUnit': FloatType, allows nulls\n",
    "# - 'Expenses': FloatType, allows nulls (previously IntegerType)\n",
    "# - '_corrupt_record': StringType, allows nulls (previously FloatType) to handle corrupt or malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a1ec4a-c417-479a-97d1-684dabfdd35f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permissive Mode"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>EmpID</th><th>EmpCount</th><th>ProductionUnit</th><th>Expenses</th><th>_corrupt_record</th></tr></thead><tbody><tr><td>January</td><td>1</td><td>78</td><td>3245.76</td><td>1125.43</td><td>null</td></tr><tr><td>February</td><td>2</td><td>93</td><td>4678.24</td><td>1458.21</td><td>null</td></tr><tr><td>March</td><td>3</td><td>57</td><td>1987.91</td><td>892.34</td><td>null</td></tr><tr><td>April</td><td>4</td><td>65</td><td>3562.51</td><td>1234.78</td><td>null</td></tr><tr><td>May</td><td>5</td><td>82</td><td>2894.37</td><td>987.65</td><td>null</td></tr><tr><td>June</td><td>6</td><td>74</td><td>1245.89</td><td>765.43</td><td>null</td></tr><tr><td>July</td><td>7</td><td>68</td><td>3987.32</td><td>null</td><td>July,7,68,3987.32,hi</td></tr><tr><td>August</td><td>8</td><td>89</td><td>2312.45</td><td>1100.56</td><td>null</td></tr><tr><td>September</td><td>9</td><td>73</td><td>1789.34</td><td>800.21</td><td>null</td></tr><tr><td>October</td><td>10</td><td>96</td><td>4321.76</td><td>1650.32</td><td>null</td></tr><tr><td>November</td><td>11</td><td>53</td><td>1345.78</td><td>700.43</td><td>null</td></tr><tr><td>December</td><td>12</td><td>61</td><td>2789.12</td><td>1200.87</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "January",
         1,
         78,
         3245.76,
         1125.43,
         null
        ],
        [
         "February",
         2,
         93,
         4678.24,
         1458.21,
         null
        ],
        [
         "March",
         3,
         57,
         1987.91,
         892.34,
         null
        ],
        [
         "April",
         4,
         65,
         3562.51,
         1234.78,
         null
        ],
        [
         "May",
         5,
         82,
         2894.37,
         987.65,
         null
        ],
        [
         "June",
         6,
         74,
         1245.89,
         765.43,
         null
        ],
        [
         "July",
         7,
         68,
         3987.32,
         null,
         "July,7,68,3987.32,hi"
        ],
        [
         "August",
         8,
         89,
         2312.45,
         1100.56,
         null
        ],
        [
         "September",
         9,
         73,
         1789.34,
         800.21,
         null
        ],
        [
         "October",
         10,
         96,
         4321.76,
         1650.32,
         null
        ],
        [
         "November",
         11,
         53,
         1345.78,
         700.43,
         null
        ],
        [
         "December",
         12,
         61,
         2789.12,
         1200.87,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmpID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "EmpCount",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ProductionUnit",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "Expenses",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "_corrupt_record",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"mode\",\"PERMISSIVE\").option(\"header\",\"true\").schema(schema).load(\"/FileStore/files/emp_test-1.csv\")\n",
    "display(df) # Read a CSV file into a Spark DataFrame 'df' with a predefined schema.\n",
    "# - .format(\"csv\"): specifies that the file format is CSV\n",
    "# - .option(\"mode\", \"PERMISSIVE\"): allows Spark to load corrupt or malformed records into the '_corrupt_record' column instead of failing\n",
    "# - .option(\"header\", \"true\"): treat the first row as column headers\n",
    "# - .schema(schema): apply the custom schema defined earlier\n",
    "# - .load(\"/FileStore/files/emp_test-1.csv\"): path to the CSV file in Databricks FileStore\n",
    "# Display the DataFrame in an interactive tabular view in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ed9724f-2b8c-48b7-a8f0-8ac558f02bdf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DROPMALFORMED Mode"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>EmpID</th><th>EmpCount</th><th>ProductionUnit</th><th>Expenses</th><th>_corrupt_record</th></tr></thead><tbody><tr><td>January</td><td>1</td><td>78</td><td>3245.76</td><td>1125.43</td><td>null</td></tr><tr><td>February</td><td>2</td><td>93</td><td>4678.24</td><td>1458.21</td><td>null</td></tr><tr><td>March</td><td>3</td><td>57</td><td>1987.91</td><td>892.34</td><td>null</td></tr><tr><td>April</td><td>4</td><td>65</td><td>3562.51</td><td>1234.78</td><td>null</td></tr><tr><td>May</td><td>5</td><td>82</td><td>2894.37</td><td>987.65</td><td>null</td></tr><tr><td>June</td><td>6</td><td>74</td><td>1245.89</td><td>765.43</td><td>null</td></tr><tr><td>August</td><td>8</td><td>89</td><td>2312.45</td><td>1100.56</td><td>null</td></tr><tr><td>September</td><td>9</td><td>73</td><td>1789.34</td><td>800.21</td><td>null</td></tr><tr><td>October</td><td>10</td><td>96</td><td>4321.76</td><td>1650.32</td><td>null</td></tr><tr><td>November</td><td>11</td><td>53</td><td>1345.78</td><td>700.43</td><td>null</td></tr><tr><td>December</td><td>12</td><td>61</td><td>2789.12</td><td>1200.87</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "January",
         1,
         78,
         3245.76,
         1125.43,
         null
        ],
        [
         "February",
         2,
         93,
         4678.24,
         1458.21,
         null
        ],
        [
         "March",
         3,
         57,
         1987.91,
         892.34,
         null
        ],
        [
         "April",
         4,
         65,
         3562.51,
         1234.78,
         null
        ],
        [
         "May",
         5,
         82,
         2894.37,
         987.65,
         null
        ],
        [
         "June",
         6,
         74,
         1245.89,
         765.43,
         null
        ],
        [
         "August",
         8,
         89,
         2312.45,
         1100.56,
         null
        ],
        [
         "September",
         9,
         73,
         1789.34,
         800.21,
         null
        ],
        [
         "October",
         10,
         96,
         4321.76,
         1650.32,
         null
        ],
        [
         "November",
         11,
         53,
         1345.78,
         700.43,
         null
        ],
        [
         "December",
         12,
         61,
         2789.12,
         1200.87,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmpID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "EmpCount",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ProductionUnit",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "Expenses",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "_corrupt_record",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"mode\",\"DROPMALFORMED\").option(\"header\",\"true\").schema(schema).load(\"/FileStore/files/emp_test-1.csv\")\n",
    "display(df) # Read a CSV file into a Spark DataFrame 'df' with a predefined schema, dropping malformed records.\n",
    "# - .format(\"csv\"): specifies that the file format is CSV\n",
    "# - .option(\"mode\", \"DROPMALFORMED\"): skip any rows that do not match the schema instead of loading them\n",
    "# - .option(\"header\", \"true\"): treat the first row as column headers\n",
    "# - .schema(schema): apply the custom schema defined earlier\n",
    "# - .load(\"/FileStore/files/emp_test-1.csv\"): path to the CSV file in Databricks FileStore\n",
    "# Display the DataFrame in an interactive tabular view in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db792938-621c-45b8-8d67-c7362341cc9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09 - Bad Records Handling",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}